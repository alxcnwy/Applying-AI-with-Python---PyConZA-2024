{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9b4838-27d8-4885-b046-d15ca57ef65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f0887a-ca09-45f4-aba4-b17181937bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing talk 1/36: https://2024.za.pycon.org/talks/11-applying-ai-with-python/\n",
      "Processing talk 2/36: https://2024.za.pycon.org/talks/13-harnessing-the-power-of-community-lessons-from-speedrunning-for-the-python-ecosystem-and-beyond/\n",
      "Processing talk 3/36: https://2024.za.pycon.org/talks/19-its-about-time-time-series-forecasting-with-darts/\n",
      "Processing talk 4/36: https://2024.za.pycon.org/talks/20-monitoring-and-evaluating-llm-apps-with-langfuse/\n",
      "Processing talk 5/36: https://2024.za.pycon.org/talks/21-leveraging-the-nltk-library-for-translation-a-case-study-of-dyula-french-translation/\n",
      "Processing talk 6/36: https://2024.za.pycon.org/talks/22-creating-personalised-images-with-pythons-stable-diffusion/\n",
      "Processing talk 7/36: https://2024.za.pycon.org/talks/27-mental-illness-and-vulnerability-in-tech/\n",
      "Processing talk 8/36: https://2024.za.pycon.org/talks/32-bridging-language-barriers-making-programming-education-accessible-to-all/\n",
      "Processing talk 9/36: https://2024.za.pycon.org/talks/33-level-up-your-developer-experience-with-kubernetes/\n",
      "Processing talk 10/36: https://2024.za.pycon.org/talks/35-a-novel-way-of-creating-multi-tenant-apps-using-django-no-third-party-packages-apply/\n",
      "Processing talk 11/36: https://2024.za.pycon.org/talks/37-building-a-decisioning-engine-for-data-scientists-a-practical-guide/\n",
      "Processing talk 12/36: https://2024.za.pycon.org/talks/38-implementing-an-hpy-backend-for-cython/\n",
      "Processing talk 13/36: https://2024.za.pycon.org/talks/39-streamlining-hpc-operations-integrating-django-helpdesk-in-resource-constrained-environments/\n",
      "Processing talk 14/36: https://2024.za.pycon.org/talks/40-you-dont-need-a-data-service-you-just-need-an-object-store-and-some-json-files/\n",
      "Processing talk 15/36: https://2024.za.pycon.org/talks/41-great-expectations-about-data-quality/\n",
      "Processing talk 16/36: https://2024.za.pycon.org/talks/42-made-you-look-using-siamese-neural-networks-for-building-change-detection-at-the-city-of-cape-town/\n",
      "Processing talk 17/36: https://2024.za.pycon.org/talks/43-kafka-in-practice-lessons-learned-at-takealot/\n",
      "Processing talk 18/36: https://2024.za.pycon.org/talks/46-power-to-the-people-who-teach-the-people-to-code/\n",
      "Processing talk 19/36: https://2024.za.pycon.org/talks/49-adventures-in-garbage-collection/\n",
      "Processing talk 20/36: https://2024.za.pycon.org/talks/51-robotics-for-all/\n",
      "Processing talk 21/36: https://2024.za.pycon.org/talks/52-maintaining-a-plc-communication-library-called-python-snap7-without-owning-a-plc/\n",
      "Processing talk 22/36: https://2024.za.pycon.org/talks/53-duck-duck-python-olap-data-with-duckdb/\n",
      "Processing talk 23/36: https://2024.za.pycon.org/talks/23-differentiation-engines-the-elves-behind-the-ai-christmas/\n",
      "Processing talk 24/36: https://2024.za.pycon.org/talks/25-accelerate-your-pandas-workload-using-fireducks-at-zero-manual-effort/\n",
      "Processing talk 25/36: https://2024.za.pycon.org/talks/31-sensor-data-processing-on-microcontrollers-with-micropython/\n",
      "Processing talk 26/36: https://2024.za.pycon.org/talks/34-building-a-thriving-tech-community/\n",
      "Processing talk 27/36: https://2024.za.pycon.org/talks/50-building-a-code-search-engine-using-nlp-to-find-similar-methods-across-libraries/\n",
      "Processing talk 28/36: https://2024.za.pycon.org/talks/55-beyond-the-pause-exploring-the-inner-workings-of-pythons-sleep/\n",
      "Processing talk 29/36: https://2024.za.pycon.org/talks/15-exploring-the-adoption-role-of-open-source-software-in-schools/\n",
      "Processing talk 30/36: https://2024.za.pycon.org/talks/30-using-coding-skills-to-make-passive-income/\n",
      "Processing talk 31/36: https://2024.za.pycon.org/talks/26-modern-web-frontend-development-with-python-htmx-and-friends/\n",
      "Processing talk 32/36: https://2024.za.pycon.org/talks/48-modern-web-frontend-development-with-python-htmx-and-friends-part-2/\n",
      "Processing talk 33/36: https://2024.za.pycon.org/talks/58-pycon-africa-pyconza-2025-discussion/\n",
      "Processing talk 34/36: https://2024.za.pycon.org/talks/56-community-contributions/\n",
      "Processing talk 35/36: https://2024.za.pycon.org/talks/57-professional-software-isnt/\n",
      "Processing talk 36/36: https://2024.za.pycon.org/talks/59-software-development-at-the-south-african-radio-astronomy-observatory/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Base URLs\n",
    "MAIN_URL = 'https://2024.za.pycon.org/talks/'\n",
    "BASE_URL = 'https://2024.za.pycon.org'\n",
    "\n",
    "# Directories to save the files\n",
    "TXT_SAVE_DIR = 'Text/data/talks/'\n",
    "CSV_SAVE_DIR = 'Text/data/'\n",
    "SPEAKER_SAVE_DIR = 'Text/data/speakers/'\n",
    "COMBINED_SAVE_DIR = 'Text/data/combined/'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(TXT_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(CSV_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SPEAKER_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(COMBINED_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def get_talk_links():\n",
    "    \"\"\"Fetch the main page and extract all talk links.\"\"\"\n",
    "    response = requests.get(MAIN_URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'class': 'table table-striped'})\n",
    "    talks = []\n",
    "\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        if row.find('th'):\n",
    "            # Skip header rows\n",
    "            continue\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) < 2:\n",
    "            continue\n",
    "        title_cell = cells[0]\n",
    "        link_tag = title_cell.find('a')\n",
    "        if not link_tag:\n",
    "            continue\n",
    "        link = link_tag['href']\n",
    "        talks.append(BASE_URL + link)\n",
    "    return talks\n",
    "\n",
    "def parse_talk_page(talk_url, talk_id, speaker_id):\n",
    "    \"\"\"Fetch and parse a talk page to extract metadata and speaker information.\"\"\"\n",
    "    response = requests.get(talk_url)\n",
    "    talk_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    section = talk_soup.find('section', {'class': 'wafer wafer-talk'})\n",
    "    if not section:\n",
    "        return None\n",
    "\n",
    "    # Extract title\n",
    "    title = section.find('h1').get_text(strip=True)\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata_div = section.find('div')\n",
    "    metadata_p = metadata_div.find_all('p')\n",
    "    metadata = {}\n",
    "\n",
    "    speaker_info = {}\n",
    "    speaker_name = None\n",
    "    speaker_profile_url = None\n",
    "    for p in metadata_p:\n",
    "        text = p.get_text(separator=' ', strip=True)\n",
    "        if ':' in text:\n",
    "            key, value = text.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            # Handle the 'Speaker' field separately\n",
    "            if key == 'Speaker':\n",
    "                speaker_tag = p.find('a')\n",
    "                if speaker_tag:\n",
    "                    speaker_name = speaker_tag.get_text(strip=True)\n",
    "                    speaker_profile_url = BASE_URL + speaker_tag['href']\n",
    "                    # Parse speaker page for speaker metadata\n",
    "                    speaker_info = parse_speaker_page(speaker_profile_url)\n",
    "                    speaker_info['Name'] = speaker_name\n",
    "                    speaker_info['Profile URL'] = speaker_profile_url\n",
    "                    speaker_info['Talk ID'] = talk_id  # Link speaker to the talk\n",
    "                    speaker_info['Speaker ID'] = speaker_id  # Add speaker ID to their data\n",
    "            metadata[key] = value\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract_div = section.find('div', {'id': 'abstract'})\n",
    "    abstract = abstract_div.get_text(separator='\\n', strip=True) if abstract_div else ''\n",
    "\n",
    "    # Combine all data\n",
    "    talk_data = {\n",
    "        'Talk ID': talk_id,\n",
    "        'Title': title,\n",
    "        'Abstract': abstract,\n",
    "        'Speaker Name': speaker_name,\n",
    "        'Speaker Profile URL': speaker_profile_url,\n",
    "        'Talk URL': talk_url,\n",
    "        'Speaker ID': speaker_id  # Include speaker ID in the talk data\n",
    "    }\n",
    "    \n",
    "    return talk_data, speaker_info\n",
    "\n",
    "def parse_speaker_page(speaker_url):\n",
    "    \"\"\"Fetch and parse a speaker's page to extract metadata.\"\"\"\n",
    "    response = requests.get(speaker_url)\n",
    "    speaker_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    speaker_data = {}\n",
    "\n",
    "    # Extract speaker profile photo URL\n",
    "    photo_tag = speaker_soup.find('img', {'class': 'img-circle'})\n",
    "    speaker_data['Photo URL'] = BASE_URL + photo_tag['src'] if photo_tag else 'No photo available'\n",
    "\n",
    "    # Extract social links and other metadata\n",
    "    bio_section = speaker_soup.find('section', {'class': 'wafer-profile-bio'})\n",
    "    if bio_section:\n",
    "        bio_links = bio_section.find_all('a')\n",
    "        for link in bio_links:\n",
    "            url = link['href']\n",
    "            if 'twitter' in url:\n",
    "                speaker_data['Twitter'] = url\n",
    "            elif 'github' in url:\n",
    "                speaker_data['GitHub'] = url\n",
    "            elif 'fosstodon' in url:\n",
    "                speaker_data['Fediverse'] = url\n",
    "\n",
    "        # Split bio into parts and save as separate fields\n",
    "        bio_paragraphs = bio_section.find_all('p')\n",
    "        for i, bio_part in enumerate(bio_paragraphs, start=1):\n",
    "            speaker_data[f'Bio Part {i}'] = bio_part.get_text(strip=True)\n",
    "\n",
    "    return speaker_data\n",
    "\n",
    "def save_talk_as_txt(talk_data, talk_id):\n",
    "    \"\"\"Save a talk's metadata and abstract as a .txt file using the talk ID.\"\"\"\n",
    "    file_path = os.path.join(TXT_SAVE_DIR, f\"{talk_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Talk ID: {talk_id}\\n\")  # Include talk ID\n",
    "        file.write(f\"Speaker ID: {talk_data.get('Speaker ID')}\\n\")  # Include speaker ID\n",
    "        # Write metadata at the top\n",
    "        for key, value in talk_data.items():\n",
    "            if key != 'Abstract' and key != 'Speaker ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "        file.write(\"\\nAbstract:\\n\")\n",
    "        file.write(talk_data.get('Abstract', 'No abstract available'))\n",
    "\n",
    "def save_speaker_as_txt(speaker_data, speaker_id):\n",
    "    \"\"\"Save a speaker's data as a .txt file using the speaker's ID.\"\"\"\n",
    "    file_path = os.path.join(SPEAKER_SAVE_DIR, f\"{speaker_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Speaker ID: {speaker_id}\\n\")  # Include speaker ID\n",
    "        file.write(f\"Talk ID: {speaker_data.get('Talk ID')}\\n\")  # Include talk ID\n",
    "        for key, value in speaker_data.items():\n",
    "            if key != 'Talk ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def save_combined_as_txt(talk_data, speaker_data, talk_id):\n",
    "    \"\"\"Save a combined .txt file with both talk and speaker data.\"\"\"\n",
    "    file_path = os.path.join(COMBINED_SAVE_DIR, f\"combined_{talk_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Talk ID: {talk_id}\\n\")  # Include talk ID\n",
    "        file.write(f\"Speaker ID: {talk_data.get('Speaker ID')}\\n\")  # Include speaker ID\n",
    "        file.write(\"\\n--- Talk Data ---\\n\")\n",
    "        for key, value in talk_data.items():\n",
    "            if key != 'Abstract' and key != 'Speaker ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "        file.write(\"\\nAbstract:\\n\")\n",
    "        file.write(talk_data.get('Abstract', 'No abstract available'))\n",
    "        \n",
    "        file.write(\"\\n--- Speaker Data ---\\n\")\n",
    "        for key, value in speaker_data.items():\n",
    "            if key != 'Talk ID' and key != 'Speaker ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def save_combined_as_csv(talk_data, speaker_data):\n",
    "    \"\"\"Return a single dictionary combining both talk and speaker data for CSV output.\"\"\"\n",
    "    combined_data = {}\n",
    "    combined_data.update(talk_data)\n",
    "    combined_data.update(speaker_data)\n",
    "    return combined_data\n",
    "\n",
    "# Scrape talks and speakers\n",
    "talk_links = get_talk_links()\n",
    "all_talks = []\n",
    "all_speakers = []\n",
    "combined_data_list = []  # Stores the combined data for all talks and speakers\n",
    "speaker_id_counter = 1  # Initialize speaker ID counter\n",
    "\n",
    "for talk_id, link in enumerate(talk_links, start=1):\n",
    "    print(f\"Processing talk {talk_id}/{len(talk_links)}: {link}\")\n",
    "    talk_data, speaker_data = parse_talk_page(link, talk_id, speaker_id_counter)\n",
    "    \n",
    "    if talk_data:\n",
    "        all_talks.append(talk_data)\n",
    "        save_talk_as_txt(talk_data, talk_id)  # Save each talk with ID as filename\n",
    "    \n",
    "    if speaker_data:\n",
    "        speaker_data['Speaker ID'] = speaker_id_counter  # Add speaker ID\n",
    "        all_speakers.append(speaker_data)\n",
    "        save_speaker_as_txt(speaker_data, speaker_id_counter)  # Save speaker data with their ID as filename\n",
    "        \n",
    "        # Combine data for CSV and txt files\n",
    "        save_combined_as_txt(talk_data, speaker_data, talk_id)\n",
    "        combined_data_list.append(save_combined_as_csv(talk_data, speaker_data))\n",
    "        \n",
    "        speaker_id_counter += 1\n",
    "\n",
    "# Define CSV columns for talk and speaker data\n",
    "speaker_fieldnames = ['Speaker ID', 'Name', 'Profile URL', 'Photo URL', 'Twitter', 'GitHub', 'Fediverse', 'Bio Part 1', 'Bio Part 2', 'Bio Part 3']\n",
    "talk_fieldnames = ['Talk ID', 'Title', 'Abstract', 'Speaker Name', 'Speaker Profile URL', 'Talk URL', 'Speaker ID']\n",
    "\n",
    "# Write speaker data to speaker.csv\n",
    "speaker_csv_path = os.path.join(CSV_SAVE_DIR, 'speakers.csv')\n",
    "with open(speaker_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=speaker_fieldnames)\n",
    "    writer.writeheader()\n",
    "    for speaker in all_speakers:\n",
    "        writer.writerow({key: speaker.get(key, None) for key in speaker_fieldnames})\n",
    "\n",
    "# Write talk data to talks.csv\n",
    "talk_csv_path = os.path.join(CSV_SAVE_DIR, 'talks.csv')\n",
    "with open(talk_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=talk_fieldnames)\n",
    "    writer.writeheader()\n",
    "    for talk in all_talks:\n",
    "        writer.writerow({key: talk.get(key, None) for key in talk_fieldnames})\n",
    "\n",
    "# Write combined data to combined.csv\n",
    "combined_csv_path = os.path.join(CSV_SAVE_DIR, 'combined.csv')\n",
    "with open(combined_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=talk_fieldnames + speaker_fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in combined_data_list:\n",
    "        writer.writerow({key: row.get(key, None) for key in talk_fieldnames + speaker_fieldnames})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c20df-7e1f-4a9e-add7-b86d50b4f092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
