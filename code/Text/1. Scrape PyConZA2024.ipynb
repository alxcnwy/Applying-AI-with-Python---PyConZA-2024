{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e10f8dc-29a7-4b67-abac-d6dfbe4930ef",
   "metadata": {},
   "source": [
    "# Text - Scrape PyConZA2024 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38729acb-2be8-47da-9e44-cc7cf7c2763e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c81a72-f889-4af0-a573-87e33e54f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b093c58-649a-4eba-b5be-ba33d15029f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URLs\n",
    "MAIN_URL = 'https://2024.za.pycon.org/talks/'\n",
    "BASE_URL = 'https://2024.za.pycon.org'\n",
    "\n",
    "# Directories to save the files\n",
    "TXT_SAVE_DIR = 'Text/data/talks/'\n",
    "CSV_SAVE_DIR = 'Text/data/'\n",
    "SPEAKER_SAVE_DIR = 'Text/data/speakers/'\n",
    "COMBINED_SAVE_DIR = 'Text/data/combined/'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(TXT_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(CSV_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SPEAKER_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(COMBINED_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ba39e-c463-4b1d-a2ac-5cf340ed589d",
   "metadata": {},
   "source": [
    "# Helper functions for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd36a1-9787-4e35-9983-af2be23738e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_talk_links():\n",
    "    \"\"\"Fetch the main page and extract all talk links.\"\"\"\n",
    "    response = requests.get(MAIN_URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'class': 'table table-striped'})\n",
    "    talks = []\n",
    "\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        if row.find('th'):\n",
    "            # Skip header rows\n",
    "            continue\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) < 2:\n",
    "            continue\n",
    "        title_cell = cells[0]\n",
    "        link_tag = title_cell.find('a')\n",
    "        if not link_tag:\n",
    "            continue\n",
    "        link = link_tag['href']\n",
    "        talks.append(BASE_URL + link)\n",
    "    return talks\n",
    "\n",
    "def parse_talk_page(talk_url, talk_id, speaker_id):\n",
    "    \"\"\"Fetch and parse a talk page to extract metadata and speaker information.\"\"\"\n",
    "    response = requests.get(talk_url)\n",
    "    talk_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    section = talk_soup.find('section', {'class': 'wafer wafer-talk'})\n",
    "    if not section:\n",
    "        return None\n",
    "\n",
    "    # Extract title\n",
    "    title = section.find('h1').get_text(strip=True)\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata_div = section.find('div')\n",
    "    metadata_p = metadata_div.find_all('p')\n",
    "    metadata = {}\n",
    "\n",
    "    speaker_info = {}\n",
    "    speaker_name = None\n",
    "    speaker_profile_url = None\n",
    "    for p in metadata_p:\n",
    "        text = p.get_text(separator=' ', strip=True)\n",
    "        if ':' in text:\n",
    "            key, value = text.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            # Handle the 'Speaker' field separately\n",
    "            if key == 'Speaker':\n",
    "                speaker_tag = p.find('a')\n",
    "                if speaker_tag:\n",
    "                    speaker_name = speaker_tag.get_text(strip=True)\n",
    "                    speaker_profile_url = BASE_URL + speaker_tag['href']\n",
    "                    # Parse speaker page for speaker metadata\n",
    "                    speaker_info = parse_speaker_page(speaker_profile_url)\n",
    "                    speaker_info['Name'] = speaker_name\n",
    "                    speaker_info['Profile URL'] = speaker_profile_url\n",
    "                    speaker_info['Talk ID'] = talk_id  # Link speaker to the talk\n",
    "                    speaker_info['Speaker ID'] = speaker_id  # Add speaker ID to their data\n",
    "            metadata[key] = value\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract_div = section.find('div', {'id': 'abstract'})\n",
    "    abstract = abstract_div.get_text(separator='\\n', strip=True) if abstract_div else ''\n",
    "\n",
    "    # Combine all data\n",
    "    talk_data = {\n",
    "        'Talk ID': talk_id,\n",
    "        'Title': title,\n",
    "        'Abstract': abstract,\n",
    "        'Speaker Name': speaker_name,\n",
    "        'Speaker Profile URL': speaker_profile_url,\n",
    "        'Talk URL': talk_url,\n",
    "        'Speaker ID': speaker_id  # Include speaker ID in the talk data\n",
    "    }\n",
    "    \n",
    "    return talk_data, speaker_info\n",
    "\n",
    "def parse_speaker_page(speaker_url):\n",
    "    \"\"\"Fetch and parse a speaker's page to extract metadata.\"\"\"\n",
    "    response = requests.get(speaker_url)\n",
    "    speaker_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    speaker_data = {}\n",
    "\n",
    "    # Extract speaker profile photo URL\n",
    "    photo_tag = speaker_soup.find('img', {'class': 'img-circle'})\n",
    "    speaker_data['Photo URL'] = BASE_URL + photo_tag['src'] if photo_tag else 'No photo available'\n",
    "\n",
    "    # Extract social links and other metadata\n",
    "    bio_section = speaker_soup.find('section', {'class': 'wafer-profile-bio'})\n",
    "    if bio_section:\n",
    "        bio_links = bio_section.find_all('a')\n",
    "        for link in bio_links:\n",
    "            url = link['href']\n",
    "            if 'twitter' in url:\n",
    "                speaker_data['Twitter'] = url\n",
    "            elif 'github' in url:\n",
    "                speaker_data['GitHub'] = url\n",
    "            elif 'fosstodon' in url:\n",
    "                speaker_data['Fediverse'] = url\n",
    "\n",
    "        # Split bio into parts and save as separate fields\n",
    "        bio_paragraphs = bio_section.find_all('p')\n",
    "        for i, bio_part in enumerate(bio_paragraphs, start=1):\n",
    "            speaker_data[f'Bio Part {i}'] = bio_part.get_text(strip=True)\n",
    "\n",
    "    return speaker_data\n",
    "\n",
    "def save_talk_as_txt(talk_data, talk_id):\n",
    "    \"\"\"Save a talk's metadata and abstract as a .txt file using the talk ID.\"\"\"\n",
    "    file_path = os.path.join(TXT_SAVE_DIR, f\"{talk_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Talk ID: {talk_id}\\n\")  # Include talk ID\n",
    "        file.write(f\"Speaker ID: {talk_data.get('Speaker ID')}\\n\")  # Include speaker ID\n",
    "        # Write metadata at the top\n",
    "        for key, value in talk_data.items():\n",
    "            if key != 'Abstract' and key != 'Speaker ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "        file.write(\"\\nAbstract:\\n\")\n",
    "        file.write(talk_data.get('Abstract', 'No abstract available'))\n",
    "\n",
    "def save_speaker_as_txt(speaker_data, speaker_id):\n",
    "    \"\"\"Save a speaker's data as a .txt file using the speaker's ID.\"\"\"\n",
    "    file_path = os.path.join(SPEAKER_SAVE_DIR, f\"{speaker_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Speaker ID: {speaker_id}\\n\")  # Include speaker ID\n",
    "        file.write(f\"Talk ID: {speaker_data.get('Talk ID')}\\n\")  # Include talk ID\n",
    "        for key, value in speaker_data.items():\n",
    "            if key != 'Talk ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def save_combined_as_txt(talk_data, speaker_data, talk_id):\n",
    "    \"\"\"Save a combined .txt file with both talk and speaker data.\"\"\"\n",
    "    file_path = os.path.join(COMBINED_SAVE_DIR, f\"combined_{talk_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Talk ID: {talk_id}\\n\")  # Include talk ID\n",
    "        file.write(f\"Speaker ID: {talk_data.get('Speaker ID')}\\n\")  # Include speaker ID\n",
    "        file.write(\"\\n--- Talk Data ---\\n\")\n",
    "        for key, value in talk_data.items():\n",
    "            if key != 'Abstract' and key != 'Speaker ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "        file.write(\"\\nAbstract:\\n\")\n",
    "        file.write(talk_data.get('Abstract', 'No abstract available'))\n",
    "        \n",
    "        file.write(\"\\n--- Speaker Data ---\\n\")\n",
    "        for key, value in speaker_data.items():\n",
    "            if key != 'Talk ID' and key != 'Speaker ID':\n",
    "                file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def save_combined_as_csv(talk_data, speaker_data):\n",
    "    \"\"\"Return a single dictionary combining both talk and speaker data for CSV output.\"\"\"\n",
    "    combined_data = {}\n",
    "    combined_data.update(talk_data)\n",
    "    combined_data.update(speaker_data)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4786696-6c63-45a2-aca4-7485780a8ec1",
   "metadata": {},
   "source": [
    "# Scrape Talks & Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cd6c8-ed6a-456a-9c5a-587c22d1af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape talks and speakers\n",
    "talk_links = get_talk_links()\n",
    "all_talks = []\n",
    "all_speakers = []\n",
    "combined_data_list = []  # Stores the combined data for all talks and speakers\n",
    "speaker_id_counter = 1  # Initialize speaker ID counter\n",
    "\n",
    "for talk_id, link in enumerate(talk_links, start=1):\n",
    "    print(f\"Processing talk {talk_id}/{len(talk_links)}: {link}\")\n",
    "    talk_data, speaker_data = parse_talk_page(link, talk_id, speaker_id_counter)\n",
    "    \n",
    "    if talk_data:\n",
    "        all_talks.append(talk_data)\n",
    "        save_talk_as_txt(talk_data, talk_id)  # Save each talk with ID as filename\n",
    "    \n",
    "    if speaker_data:\n",
    "        speaker_data['Speaker ID'] = speaker_id_counter  # Add speaker ID\n",
    "        all_speakers.append(speaker_data)\n",
    "        save_speaker_as_txt(speaker_data, speaker_id_counter)  # Save speaker data with their ID as filename\n",
    "        \n",
    "        # Combine data for CSV and txt files\n",
    "        save_combined_as_txt(talk_data, speaker_data, talk_id)\n",
    "        combined_data_list.append(save_combined_as_csv(talk_data, speaker_data))\n",
    "        \n",
    "        speaker_id_counter += 1\n",
    "\n",
    "# Define CSV columns for talk and speaker data\n",
    "speaker_fieldnames = ['Speaker ID', 'Name', 'Profile URL', 'Photo URL', 'Twitter', 'GitHub', 'Fediverse', 'Bio Part 1', 'Bio Part 2', 'Bio Part 3']\n",
    "talk_fieldnames = ['Talk ID', 'Title', 'Abstract', 'Speaker Name', 'Speaker Profile URL', 'Talk URL', 'Speaker ID']\n",
    "\n",
    "# Write speaker data to speaker.csv\n",
    "speaker_csv_path = os.path.join(CSV_SAVE_DIR, 'speakers.csv')\n",
    "with open(speaker_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=speaker_fieldnames)\n",
    "    writer.writeheader()\n",
    "    for speaker in all_speakers:\n",
    "        writer.writerow({key: speaker.get(key, None) for key in speaker_fieldnames})\n",
    "\n",
    "# Write talk data to talks.csv\n",
    "talk_csv_path = os.path.join(CSV_SAVE_DIR, 'talks.csv')\n",
    "with open(talk_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=talk_fieldnames)\n",
    "    writer.writeheader()\n",
    "    for talk in all_talks:\n",
    "        writer.writerow({key: talk.get(key, None) for key in talk_fieldnames})\n",
    "\n",
    "# Write combined data to combined.csv\n",
    "combined_csv_path = os.path.join(CSV_SAVE_DIR, 'combined.csv')\n",
    "with open(combined_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=talk_fieldnames + speaker_fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in combined_data_list:\n",
    "        writer.writerow({key: row.get(key, None) for key in talk_fieldnames + speaker_fieldnames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c20df-7e1f-4a9e-add7-b86d50b4f092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
